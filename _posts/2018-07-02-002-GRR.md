---
layout: post
title: 'GRR, a scary beast!'
date: 2018-07-12
categories: grr, forensics
---

# GRR, a scary beast!
GRR! Normally these words might instill fear in children, but we're hoping that we can use them to instill fear in the hearts of our adversaries. 

GRR is a tool that can be used for live, remote forensices and assist us in our forensic endavours. Let's take a high-level look at how to set up and use GRR.

As always, we're going to have Terraform code available for you to quick set this up in your AWS account or home lab. I would use something like Chef or Anisble here too for a production instance, but unless we need practice we don't need to automate the installation of a one-time demo.


## What is GRR and why would we use it?
GRR stands for *G*RR *R*apid *R*esponse and started out as a project at Google. It should be noted that it is not officially supported by Google, but is maintained/developed by 
people who work at Google and it is used internally. It is also my understanding that Google has their own internal version which slightly differs from the one us normies get access to. 

It's a tool that requires a client/server setup with each host you'd like to collect artifacts from requiring an agent. The agents use [protobuffing](https://developers.google.com/protocol-buffers/) to talk to the server, and comms are encrypted using certs that are set up during install.  

I'd also like to establish that GRR does not act as a detection platform, but is strictly focused on response. When you think (or know) that a host is infected, GRR solves the problem of how do you quickly search for 
specific files, registry keys, or any other artifacts we might collect. Did I mention that you can do this remotely? That's what makes this ideal for the modern organization where you might have endpoints
that are scattered throuought the globe and not in central offices anymore. To me - this was the main draw. The ability to be able to quickly and remotely not only collect, but also scan my infrastructure to look for specific artifacts.
Once we can confirm one infection, GRR really shines when you can scan the rest of your fleet to see if the same indicator exists elsewhere.  

Lastly, the team behind GRR have attempted to put together one thorough list of artifacts that are on Windows, MacOS, and *nix hosts. This helps us all as then we have one repository to browse to get all of the artifacts and what their
significance to our investigation might be.


## GRR component overview 
GRR can be set up on a standalone machine for development and testing purposes (which we'll be doing today) but it is also possible to split the components and have them on seperate hardware. You can find a great tutorial on that over [here](https://jessicawilson.us/blog/15 ) on Jessica Wilson's blog. 

The components that make up GRR are:
  - The admin UI
      This is the UI that you'll be connecting to and doing most of the work from.

  - The frontend
      Clients will comminucate with the frontend, and tasks kicked off by you in the admin UI will be sent to clients through the frontend.

  - Workers
      Workers in GRR are what actually process the data behind the scenes - meaning that tasks that the frontend hands out are actually prepped, shipped and received by the workers.

  - Datastore
      This is the heart of the operation. Any tasks you queue up from the admin UI get recorded in the datastore for workers to come and process. The datastore also stores any of the data captured. 

Each of these components can be invoked seprately through the `grr_server --component` command. 

## How to install GRR
A quick note on how I'm setting this up.

As is tradition, we will be doing everything on AWS and using Terraform. 

In AWS, we will be:
  - Setting up a public/private VPC that contains one private and one public subnet
  - Using an EC2 instance in the public subnet to act as a bastion host that we will connect to from our house
  - Using EC2 instances within the private subnet to be our GRR server(s) and test subjects, which we can access from the bastion host

There are multiple ways to set up the bastion host, but for simplicities sake we will just set up a Windows node that we will RDP into for now. In future labs, we will be going over some fancier SSH tunneling business so we can get to our hosts a little quicker.

Now that we how we're setting up our infrastrucutre - let's continue with the install of GRR. This is actually very straight forward to set up and thanks to the people who work on GRR there is now a `.deb` that we can install.

Installing the package is as simple as following the [docs](https://grr-doc.readthedocs.io/en/latest/installing-grr-server/from-release-deb.html):

  1. Check the latest released version in the [repo](https://github.com/google/grr/releases)

  2. Use the latest release version in the URI of the path to the package
      Example to get the latest release a time of writing (3.2.2.0):
        `wget https://storage.googleapis.com/releases.grr-response.com/grr-server_3.2.2-0_amd64.deb`

  3. After the package has been downloaded, use `apt` to install it
      `sudo apt install ./grr-server-3.2.2-0_amd64.deb`

  4. As you run through the install process, you will be asked some questions about what components that you'd like to use for your installation. You can read a paper [here](https://www.sciencedirect.com/science/article/pii/S1742287615000171) on the performance impacts of your decisions. Other than the hostname and email domain, everything for me will have just been set up using the default options. 

  5. This is a good point to mention that we'll need to update our security groups in AWS to allow the new ports to communicate among our instances. This is already done by Terraform for you, but just remember about this in the future. 

  6. After that, you should be good to go! Open your browser and go login with the admin credentials you specified on the admin interface at http://grr-server:8000

  <img src="/post-files/002/img/1-adminui.png" alt="GRR Admin UI" style="height:auto;width:650px;"/>

  Boom! Look at that. It's all set up... yet so empty. 

**NOTE:** If you are setting up a production envrionment, make sure that you follow the [Securing Access](https://grr-doc.readthedocs.io/en/latest/installing-grr-server/securing-access.html) component in the GRR docs before doing anything else.

## How to use GRR

Now that we have a server set up, let's install some clients. Installation is quick and painless and can be distributed to client machines the same way you distribute your software now. It's just a pre-packaged binary that you distribute.

We will be manually installing the agent on our Windows server (the one that's sitting in the private subnet). 

 1. Log in to the admin UI on the GRR server.

 2. On the sidebar, go to `Configuration` > `Manage Binaries`

 3. Choose the appropriate binary, and click to download it. I will be downloading the Windows installer (without `dbg` in the name).

 4. Once downloaded, log in to the machine you'd like to install it on and run the binary. If you have a mechanism to remotely install software on your fleet you can do it that way too.

 5. Run the binary.

 6. That's it! The agent will connect back to the GRR server on port set during install.

It may take a few minutes to connect back, and we can validate that it has by using the admin UI and searching for something like `ip:10.0` or whatever your subnet starts with. 

As you can see, we have our agent connecting back to our server here:

  <img src="/post-files/002/img/2-clients.png" alt="GRR client status" style="height:auto;width:650px;"/>

Let's take a quick look at some of my favourite features (how selfish!).

### Hunts
Imagine you get a trusted piece of threat intel such as a registry key. Any host that has this specific registry key should probably be investigated further, right? How can we find that out?

This is exactly what hunts are for within GRR, we can look for specific items on the machines.

Let's walk through an example. Say I'm looking for a registry key such as `HKLM\SOFTWARE\Microsoft\Windows\CurrentVersion\Run\badValue`. We can search our fleet for this key by doing the following:
  1. From the UI, choose `Hunt Manager`.

  2. Click on `New Hunt` (the plus sign).

  3. You can see there are many different types of hunts. You can even create your own. We'll be choosing `Registry` -> `Registry Finder`.

  4. Enter the key name you'd like to search for. In our example, I entered: `HKEY_LOCAL_MACHINE/SOFTWARE/Microsoft/Windows/CurrentVersion/Run/badValue`. 

  5. You can choose a condition if you'd like to as well, in case you need to find a specific or similar value. In our exmaple I'll just leave it as is. 

  6. Choose `Next`, and then enter a `Description`. The other settings here allow us to limit the hunt so it doesn't kill resources on the server or client-side. 

  7. The output plugin you choose tells GRR what to do with the results. You can also email them if you've set that up properly. I will be leaving this as the default (BigQuery) option.

  8. Now we tell GRR where we'd like our hunt to run. This is useful if you've identified a subset of nodes you'd like to look for something on. In this case, I'll be running this rule with a `Match mode` of `Match all`, and with a `OS Type` rule looking specifically for `Windows` hosts. 

  9. We get a last glance here before the hunt settings are locked in. Once you confirm all setting appear as expected, you can click on `Create Hunt`. 

  10. From the `Hunt Manager` portion of the UI, select your newly create hunt and click on the play button to start the hunt.

  11. The hunt will then run and you can monitor the progess and results from the `Hunt Manager` tab in the UI. 

The other tabs on the hunt will allow you to check the status, which clients are pending/completed, any errors that have arisen and even infomration about resource usage this hunt is taking up on the GRR server.

By default, hunts will start off in a paused state, but you can also create hunts on a schedule if you'd like to run something more regular. 


### Collections
Collections are the bread and butter within GRR. They're what I use in order to collect artifacts from a host if I'm not looking for a specific item.

Let's a say a host has been seen browsing to a known-malicious site, or you otherwise suspect the host is compromised through other methods. A collection is something that we can use the hunt manager to run on our client in order to collect information that will help us triage what has happened. 

For example: if we have a client that is constantly reaching out to domains that look like they're part of a DGA we can run a collection on them to get information such as netstat output, running processes, browsing history, specific registry keys, prefetch, or any other information we might find useful given the scenario. 

This is done by creating YAML files that contain the names of the items you want and then saving them within your GRR server. Here's how we can do that:

  1. Create your collection YAML file. Here's an example of some preliminary data you may want to collect on a Windows host:
```
name: WindowsLightweight
doc: Everything you need for a quick analysis - processes, netstat, sched tasks, prefetch and more. Windows 7+.
sources: 
- type: ARTIFACT_GROUP
  attributes:
    names: [WMIProcessList, WMIInstalledSoftware, WindowsScheduledTasks, WindowsSuperFetchFiles, WindowsPrefetchFiles, WindowsXMLEventLogSystem, WindowsRunKeys, WindowsServices, WindowsUserDownloadsDirectory, WindowsUserRecentFiles]    
    conditions: [os_major_version >= 6]
    supported_os: [Windows]
labels: [System]
supported_os: [Windows]
```

  The attributes are all of the names of the different artifacts that are contained within GRR. There are more documents on this scattered in older GRR repos or various presentations, but I mainly created this through trial and error. Ensure that the `type` is artifact group or this will not work.

  
  2. Upload that YAML file to the GRR server through the admin UI by visiting `Artifact Manager` and choosing `Upload`.

  3. Once uploaded, start a new hunt. Choose `Collectors` -> `ArtifactCollectorFlow` -> `WindowsLightweight`. Start your hunt and the results returned will be all of the artifacts defined by your group.

  <img src="/post-files/002/img/4-collection.png" alt="Colleciton creation" style="height:auto;width:650px;"/>

  <img src="/post-files/002/img/5-collection.png" alt="Collection usage" style="height:auto;width:650px;"/>


### Troubleshooting
If you need to troubleshoot anything, it's a good idea to check the client logs to get an idea of what's going on. 

On Windows, we can find those at: `C:\Windows\System32\LogFiles\GRR.txt` and `C:\Windows\System32\LogFiles\GRR_installer.txt`

On non-Windows, we can grab the logs from: `/var/log` (there will be a few with `grr` in the name of the file. The GRR server will have a `grr` directory.). 

There are a couple of other things you can do with the client as well if you need to dig deeper:

1. By default, GRR will only log anything classified as an `ERROR` and above so you may not get the info that you need. You can stop the GRR service and edit the YAML config file to change the logging to verbose.
    This can be done by:
      - Stopping the GRR service: `net stop "grr monitor"`
      - Opening `C:\Windows\system32\GRR\GRR.exe.yaml` in a text editor
      - Find `logging.verbose: false` and change it to `logging.verbose: true`
      - Start the service: `net start "grr monitor"`

2. You can also debug the client interactively. On non-Windows platforms, you can just run the agent with the `--debug` and/or the `--verbose` flags. On Windows, you should grab a copy of the `dbg_` binary and run it locally, with the same flags.


## Conclusion
GRR is a powerful tool that is not difficult to get up and running. It is very powerful, and because of that, you must make sure that production instances are protected as much as they could be. Imagine if an adversary was able to get access to your GRR console - they could whitelist specific items of theirs or be able to look for any information they're after on all of your clients quite easily. 

We hope this was a decent spot to get started if you have never heard of GRR before. Please explore the GitHub page and documentation for more information. 


## Terraform code
**Note**: You will manually need to create two public keys before you begin, as you will use them in `ec2.tf`. 

[provider.tf](/post-files/002/tf/provider.tf) - This is created to define our provider and basic details.

[vpc.tf](/post-files/002/tf/vpc.tf) - In our VPC file, we define both our public and private subnets, as well as the infrastructure to make them internet accessible.

[sg.tf](/post-files/002/tf/sg.tf) - Our security groups become defined here. Ensure that you replace `YOUR.PUBLIC.IP.HERE` with... your public IP. 

[ec2.tf](/post-files/002/tf/ec2.tf) - Without these EC2 instances, none of this would work. Here is where they get set up. 

After an `init`, a `plan` to make sure everything looks right the lab described in this post should be up as soon as you finish running your `apply`. 
